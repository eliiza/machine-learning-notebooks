{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore tuning the hyperparameters of the Support Vector Regression algorithms using SciKit Learn's Grid Search function [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same evaluate_model function as before\n",
    "def evaluate_model(model_fn, print_result=False):\n",
    "    '''\n",
    "    Consumes a function model_fn\n",
    "    and evaluates its predictive accuracy against \n",
    "    the housing prices test set.\n",
    "    We have included a switch for the output to be a more human readable\n",
    "    printed version or the uncurtailed floating point value of the average.\n",
    "    '''\n",
    "    test_data = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/test_data.csv\")\n",
    "    actual_values = test_data['SalePrice']\n",
    "    # Pass in all columns except SalePrice\n",
    "    test_input = test_data.filter(regex='^(?!SalePrice$).*')\n",
    "    predicted_saleprice = model_fn(test_input)\n",
    "    mae = np.mean(np.abs(predicted_saleprice-actual_values))\n",
    "    if print_result:\n",
    "        return print(\"The model is inaccurate by $%.2f on average.\" % mae)\n",
    "    else:\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for data encoding\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "def encode_data(data,scaler = None):\n",
    "    \"\"\"\n",
    "    Encode a dataframe of house price data using the desired feature engineering process. \n",
    "    The scaler argument allows you to either scale the data anew (scaler = None), \n",
    "    or use previously derived scaling parameters\n",
    "    e.g. when you want to encoding test data using the scaling parameters from the training dataset.\n",
    "    Returns a dataframe of engineered features and the scaler object.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    # Numerical features\n",
    "    features = features[['OverallQual','GrLivArea','BedroomAbvGr','FullBath','YearBuilt']]\n",
    "    features['QualAreaInteract'] = features['OverallQual'] * features['GrLivArea']\n",
    "    \n",
    "    # Ordinal feature - map to numerical as before\n",
    "    cond_map = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1}\n",
    "    features['KitchenQual'] = data['KitchenQual'].map(cond_map).fillna(0)\n",
    "    \n",
    "    # Categorical features - one-hot encode using pre-written helper functions (below)\n",
    "    features['CentralAir'] = data['CentralAir'] == 'Y'\n",
    "    electrical = encode_electrical(data['Electrical'])\n",
    "    heating = encode_heating(data['Heating'])\n",
    "    features = pd.concat([features,electrical,heating],axis=1)\n",
    "    \n",
    "    # Convert to float data type for scaling process\n",
    "    features = features.astype(float)\n",
    "    \n",
    "    # Scale all the features\n",
    "    # If no `scaler` object in the function arguments - carry out scaling anew\n",
    "    # If `scaler` object in the function arguments - use those scaling parameters\n",
    "    if(not scaler):\n",
    "       scaler = MinMaxScaler()\n",
    "       scaler.fit(features)\n",
    "    features = pd.DataFrame(scaler.transform(features), \n",
    "                            columns = ['OverallQual','GrLivArea','BedroomAbvGr','FullBath','YearBuilt',\n",
    "                                       'QualAreaInteract','KitchenQual',\n",
    "                                       'CentralAir','FuseA','FuseF','FuseP','Mix','SBrkr',\n",
    "                                       'GasA','GasW','Grav','Wall'])\n",
    "    \n",
    "    # Return the desired data frame and the scaling parameters used\n",
    "    return(features,scaler)\n",
    "\n",
    "\n",
    "# Helper functions for one hot encoding\n",
    "\n",
    "def encode_electrical(electrical):\n",
    "    \"\"\"\n",
    "    Create data frame with one column per category in 'electrical' column, rows are Boolean with respect to\n",
    "    category string in electrical column.\n",
    "    \"\"\"\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['FuseA'] = electrical == 'FuseA'\n",
    "    one_hot_encoding['FuseF'] = electrical == 'FuseF'\n",
    "    one_hot_encoding['FuseP'] = electrical == 'FuseP'\n",
    "    one_hot_encoding['Mix']   = electrical == 'Mix'\n",
    "    one_hot_encoding['SBrkr'] = electrical == 'SBrkr'\n",
    "    return(one_hot_encoding)\n",
    "\n",
    "def encode_heating(heating):\n",
    "    \"\"\"\n",
    "    Create data frame with one column per category in 'heating' column, rows are Boolean with respect to\n",
    "    category string in heating column.\n",
    "    \"\"\"\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['GasA'] = heating == 'GasA'\n",
    "    one_hot_encoding['GasW'] = heating == 'GasW'\n",
    "    one_hot_encoding['Grav'] = heating == 'Grav'\n",
    "    one_hot_encoding['Wall'] = heating == 'Wall'\n",
    "    return(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data\n",
    "training_set = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/training_data.csv\")\n",
    "\n",
    "# Feature engineering as in previous notebook (using function in utils.py)\n",
    "training_features, training_scaler = encode_data(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Support Vector Regression hyperparameters\n",
    "\n",
    "How well do the default SVR hyperparameters perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "\n",
    "# Step 2: Train the model\n",
    "predictor = sklearn.svm.SVR(gamma='auto') \n",
    "predictor.fit(training_features, training_set['SalePrice'])\n",
    "\n",
    "# Step 3: Create a function that can make predictions using the model\n",
    "def svr_model(input_data, scaler=training_scaler):\n",
    "    input_features,_ = encode_data(input_data,scaler)\n",
    "    predictions = predictor.predict(input_features)\n",
    "    return(predictions)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "evaluate_model(svr_model, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will try to tune these hyperparameters:\n",
    "\n",
    "- `C` = margin size to apply no penalty\n",
    "- `epsilon` = penalty parameter to apply to errors\n",
    "- `kernel` = type of transformation applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chosen hyperparameter values for the Grid Search\n",
    "hyperparameters = {'C':[1E5, 1E6, 1E7], 'epsilon':[100, 250, 500], \n",
    "                   'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Define the estimator to train\n",
    "svr = sklearn.svm.SVR(gamma='auto')\n",
    "\n",
    "# Define the Grid Search CV to undertake\n",
    "svr_grid_search = GridSearchCV(svr, hyperparameters, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Grid Search Cross Validation process\n",
    "svr_grid_search.fit(training_features, training_set['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what hyperparameter combination was best\n",
    "svr_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuned_svr_model(input_data):\n",
    "    \"\"\"\n",
    "    Create a function that applies the tuned SVR model to test data.\n",
    "    Returns predictions for the 'SalesPrice' column.\n",
    "    \"\"\"\n",
    "    # Create scaler for use on test data\n",
    "    training_features, scaler = encode_data(training_set)\n",
    "    # Apply scaler for feature engineering on test data\n",
    "    input_features,_ = encode_data(input_data,scaler)\n",
    "    # Predict using the test data\n",
    "    predictions = svr_grid_search.best_estimator_.predict(input_features)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "evaluate_model(tuned_svr_model, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
