{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the following data science libraries throughout this course (click the links for cheat sheets provided by DataCamp)\n",
    "* [numpy](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)  (for vectorised math operations)\n",
    "* [pandas](http://datacamp-community.s3.amazonaws.com/9f0f2ae1-8bd8-4302-a67b-e17f3059d9e8) (for dataframes)\n",
    "* [keras](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf) (for neural networks)\n",
    "* [scikitlearn](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf) (for other machine learning models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "The first step to training models is to figure out a way to tell how good a model is.  \n",
    "\n",
    "You can't just train until it \"gets the right results\", there will almost always be some difference between the predicted outcome and the measured outcomes.\n",
    "\n",
    "To tackle this, we'll split our data up into a \"training set\" (for inspection and training models), and a \"test set\" for model evaluation.\n",
    "\n",
    "This is to ensure a fair test of the model's ability to generalise to new examples. The same reason why an exam contains different questions to the practice exams a student learns from.\n",
    "\n",
    "Throughout this module, we'll be using the [Ames Housing Prices Data Set](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
    "\n",
    "The data is described [here](https://github.com/eliiza/ml-training-data/blob/master/housing_price_data/data_description.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Load data from Eliiza's github page\n",
    "labelled = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/housing_data.csv\") \n",
    "\n",
    "# Randomly shuffle the rows to avoid any ordering\n",
    "labelled = labelled.sample(frac=1.0, replace=False, random_state=2)\n",
    "\n",
    "# Split up into training and test sets (80:20).\n",
    "num_rows = labelled.shape[0]\n",
    "training_set = labelled[0:round(num_rows*0.8)]\n",
    "test_set = labelled[round(num_rows*0.8):num_rows]\n",
    "\n",
    "# If you wanted to save the training and test sets when running locally\n",
    "# training_set.to_csv(\"../data/housing_price_data/training_data.csv\",index = False)\n",
    "# test_set.to_csv(\"../data/housing_price_data/test_data.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- How many observations are now in the training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error\n",
    "\n",
    "Now that we have a test set, we can start to evaluate some models!\n",
    "\n",
    "We'll use the mean absolute error. This is the average size of the difference between the predicted value vs the observed value.\n",
    "\n",
    "Formally, this is defined as\n",
    "\n",
    "$$  \\mathsf{MAE} = \\frac{1}{n} * \\sum_{i=1}^n |\\mathsf{predicted\\_value}[i] - \\mathsf{actual\\_value}[i]|  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorised Math Operations\n",
    "\n",
    "Machine Learning makes significant use of vectorised functions (or maths with arrays) to perform various aspects of model building, and as we will see in this lab, evaluating the error of our model on our test set of data. \n",
    "\n",
    "We will be using the numpy library to do this maths for us, but it is good to understand that the operations taking place are occuring over the whole array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two one-dimensional numpy arrays (vectors)\n",
    "a = np.array([2,2,-4,6])\n",
    "b = np.array([-1,4,-3,4])\n",
    "\n",
    "print(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Using `np.abs` and `np.mean`, calculate the MAE between the predictions `a` and the observed values `b` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# Model Evaluation Workflow\n",
    "\n",
    "To help with our workflow as we try different models throughout this module, we're going to try to write functions for many of our tasks. Ask if you're not sure what any function does - we can explain!\n",
    "\n",
    "First, we write a function to evaluate how accurate any model is. It evaluates how good the target model is at predicting the Sales Price in the unseen test dataset. Its only argument is itself a function, `model_fn`. `model_fn` must be able to return Sales Price predictions when given appropriate input housing data (see below for an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_fn, print_result=False):\n",
    "    '''\n",
    "    Consumes a function model_fn\n",
    "    and evaluates its predictive accuracy against \n",
    "    the housing prices test set.\n",
    "    We have included a switch for the output to be a more human readable\n",
    "    printed version or the uncurtailed floating point value of the average.\n",
    "    '''\n",
    "    # Load test set from Eliiza's GitHub (pre-saved)\n",
    "    test_data = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/test_data.csv\")\n",
    "    actual_values = test_data['SalePrice']\n",
    "    # Pass in all columns except SalePrice\n",
    "    test_input = test_data.filter(regex='^(?!SalePrice$).*')\n",
    "    predicted_saleprice = model_fn(test_input)\n",
    "    mae = np.mean(np.abs(predicted_saleprice-actual_values))\n",
    "    if print_result:\n",
    "        return print(\"The model is inaccurate by $%.2f on average.\" % mae)\n",
    "    else:\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Let's evaluate a very simple predictive heuristic: **Sale Price = 50,000 * {Overall Quality Rating}**. `OverallQual` had a high correlation with `SalePrice` in the data exploration.\n",
    "\n",
    "So that it can be an input to `evaluate_model()`, we write it as a function which returns predictions when given some input_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_heuristic(input_data):\n",
    "    \"\"\"\n",
    "    Extracts a single vector called 'OverallQual' from input data and multiplies every value by 50,000\n",
    "    \"\"\"\n",
    "    quality = input_data['OverallQual']\n",
    "    prediction = 50000*quality\n",
    "    return(prediction)\n",
    "\n",
    "evaluate_model(quality_heuristic, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we will follow this pattern throughout this module:\n",
    "1. Write a function that contains the model we want to use. It returns predictions when given input data.\n",
    "2. Use the `evaluate_model()` function to evaluate the model from above on the unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## Exercise \n",
    "\n",
    "- Check you follow the structure and concepts behind `evaluate_model()` and `quality_heuristic()`\n",
    "- Then try to make a new version of `quality_heuristic` that achieves a lower score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make new \"Quality Heuristics\" models simply by changing the amount of dollars we multiply the number of quality rating by. Here is a function that allows us to change this value (`a`) in an automated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_heuristic(a):\n",
    "    \"\"\"\n",
    "    Creates a heuristic function with a given linear multiplier\n",
    "    \"\"\"\n",
    "    def heuristic(input_data):\n",
    "        prediction = a * input_data['OverallQual']\n",
    "        return(prediction)\n",
    "    return(heuristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is now \"What's the best amount of dollars to multiply by?\" \n",
    "\n",
    "Let's start to answer this by using a Brute Force approach - a for loop and trial and error. \n",
    "\n",
    "**Exercise:** Choose the values of `a` to try in the chunk of code below. We will loop over your values and store the accuracy of each model's attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the values of `a` to try over.\n",
    "values = [] # Insert values of a. \n",
    "# Hint: You can use range() to help. Maybe with a stepsize of $1000!\n",
    "\n",
    "# Create empty list and evaluate models between $0 and $100,000 per quality rating\n",
    "model_scores = []\n",
    "for i in values:\n",
    "    score = evaluate_model(generate_quality_heuristic(i))\n",
    "    model_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the model MAEs\n",
    "plt.plot(model_scores)\n",
    "plt.xlabel(\"Quality Score x 1000\")\n",
    "plt.ylabel(\"Mean absolute error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum score with a pandas dataframe\n",
    "models = pd.DataFrame()\n",
    "models['Score'] = model_scores\n",
    "models.loc[models.Score == models.Score.min()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with this approach is choosing an appropriate level of granularity.  \n",
    "In this example we probably stepped through increments of $1000.  \n",
    "But how do we know what the optimal value really is between multiples of 1000?  \n",
    "We could lower our step size but this would mean increasing our compute time by orders of magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (optional)\n",
    "\n",
    "Extend `evaluate_model` to report how fast the model takes to make its predictions [hint](https://docs.python.org/3.6/library/time.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
