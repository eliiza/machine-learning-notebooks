{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SciKit API\n",
    "\n",
    "\n",
    "We now have a sense of the workflow necessary to use machine learning models.\n",
    "\n",
    "The steps we take are:\n",
    "\n",
    "1. _feature engineering_: Encode all the data we want to use into numbers. Perform any other transformations you think are necessary (e.g. scaling). We end up with a 2D array `X_train` containing all the columns we want to process, as well as an array `y_train` containing the column we want to predict (possibly scaled).\n",
    "\n",
    "2. _train our model_:  This can be done with the command `model.fit(X_train,y_train)`.\n",
    "\n",
    "3. _predict_: Use our freshly trained model to make some predictions.  This is done with the command `model.predict(input)`.\n",
    "\n",
    "4. _evaluate_: We can use the `evaluate_model` function.\n",
    "\n",
    "We will now apply this workflow to different algorithms from the SciKit Learn toolkit: Gradient Boosting, Random Forests and Support Vector Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "training_set = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same evaluate_model function as before\n",
    "def evaluate_model(model_fn, print_result=False):\n",
    "    '''\n",
    "    Consumes a function model_fn\n",
    "    and evaluates its predictive accuracy against \n",
    "    the housing prices test set.\n",
    "    We have included a switch for the output to be a more human readable\n",
    "    printed version or the uncurtailed floating point value of the average.\n",
    "    '''\n",
    "    test_data = pd.read_csv(\"https://raw.githubusercontent.com/eliiza/ml-training-data/master/housing_price_data/test_data.csv\")\n",
    "    actual_values = test_data['SalePrice']\n",
    "    # Pass in all columns except SalePrice\n",
    "    test_input = test_data.filter(regex='^(?!SalePrice$).*')\n",
    "    predicted_saleprice = model_fn(test_input)\n",
    "    mae = np.mean(np.abs(predicted_saleprice-actual_values))\n",
    "    if print_result:\n",
    "        return print(\"The model is inaccurate by $%.2f on average.\" % mae)\n",
    "    else:\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Feature Engineering\n",
    "\n",
    "We are going to use some of the features we looked at in previous notebooks, initially:\n",
    "\n",
    "- Numeric:\n",
    "    - `OverallQual`\n",
    "    - `GrLivArea`\n",
    "    - `BedroomAbvGr`\n",
    "    - `FullBath`\n",
    "    - `YearBuilt`\n",
    "- Engineered:\n",
    "    - `QualAreaInteract` (from `OverallQual` and `GrLivArea`)\n",
    "- Categorical (to be one-hot encoded):\n",
    "    - `CentralAir`\n",
    "    - `Electrical`\n",
    "    - `Heating`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Review the code below - we are simply making a function to do the feature engineering steps in a repeatable manner.\n",
    "- Add any missing features that you found helped improve accuracy in the previous notebook. **For example the best ordinal variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Feature Engineering\n",
    "def encode_data(data,scaler = None):\n",
    "    \"\"\"\n",
    "    Encode a dataframe of house price data using the desired feature engineering process. \n",
    "    The scaler argument allows you to either scale the data anew (scaler = None), \n",
    "    or use previously derived scaling parameters\n",
    "    e.g. when you want to encoding test data using the scaling parameters from the training dataset.\n",
    "    Returns a dataframe of engineered features and the scaler object.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    # Numerical features\n",
    "    features = features[['OverallQual','GrLivArea','BedroomAbvGr','FullBath','YearBuilt']]\n",
    "    features['QualAreaInteract'] = features['OverallQual'] * features['GrLivArea']\n",
    "    \n",
    "    # Ordinal feature(s)\n",
    "    \n",
    "    # Categorical features - one-hot encode using pre-written helper functions (below)\n",
    "    features['CentralAir'] = data['CentralAir'] == 'Y'\n",
    "    electrical = encode_electrical(data['Electrical'])\n",
    "    heating = encode_heating(data['Heating'])\n",
    "    features = pd.concat([features,electrical,heating],axis=1)\n",
    " \n",
    "    # Convert to float data type for scaling process\n",
    "    features = features.astype(float)\n",
    "    \n",
    "    # Scale all the features\n",
    "    # If no `scaler` object in the function arguments - carry out scaling anew\n",
    "    # If `scaler` object in the function arguments - use those scaling parameters\n",
    "    if(not scaler):\n",
    "       scaler = MinMaxScaler()\n",
    "       scaler.fit(features)\n",
    "    features = pd.DataFrame(scaler.transform(features), \n",
    "                            columns = ['OverallQual','GrLivArea','BedroomAbvGr','FullBath','YearBuilt',\n",
    "                                       'QualAreaInteract',\n",
    "                                       'CentralAir',\n",
    "                                       'FuseA','FuseF','FuseP','Mix','SBrkr',\n",
    "                                       'GasA','GasW','Grav','Wall'])\n",
    "    \n",
    "    # Return the desired data frame and the scaling parameters used\n",
    "    return(features,scaler)\n",
    "\n",
    "\n",
    "# Helper functions for one hot encoding\n",
    "\n",
    "def encode_electrical(electrical):\n",
    "    \"\"\"\n",
    "    Create data frame with one column per category in 'electrical' column, rows are Boolean with respect to\n",
    "    category string in electrical column.\n",
    "    \"\"\"\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['FuseA'] = electrical == 'FuseA'\n",
    "    one_hot_encoding['FuseF'] = electrical == 'FuseF'\n",
    "    one_hot_encoding['FuseP'] = electrical == 'FuseP'\n",
    "    one_hot_encoding['Mix']   = electrical == 'Mix'\n",
    "    one_hot_encoding['SBrkr'] = electrical == 'SBrkr'\n",
    "    return(one_hot_encoding)\n",
    "\n",
    "def encode_heating(heating):\n",
    "    \"\"\"\n",
    "    Create data frame with one column per category in 'heating' column, rows are Boolean with respect to\n",
    "    category string in heating column.\n",
    "    \"\"\"\n",
    "    one_hot_encoding = pd.DataFrame()\n",
    "    one_hot_encoding['GasA'] = heating == 'GasA'\n",
    "    one_hot_encoding['GasW'] = heating == 'GasW'\n",
    "    one_hot_encoding['Grav'] = heating == 'Grav'\n",
    "    one_hot_encoding['Wall'] = heating == 'Wall'\n",
    "    return(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Gradient Boosting\n",
    "\n",
    "We will be using SciKit Learn's Gradient Boosting implementation: [sklearn.ensemble.GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html). There are other implementations of all of these algorithms which we won't use here, including [XGBoost](https://xgboost.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Feature engineering\n",
    "training_features, training_scaler = encode_data(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting \n",
    "# Step 2: Train the model\n",
    "predictor = sklearn.ensemble.GradientBoostingRegressor()\n",
    "predictor.fit(training_features, training_set['SalePrice'])\n",
    "    \n",
    "# Step 3: Create a function that can make predictions using the above trained model and scaler\n",
    "def boosting_model(input_data, scaler = training_scaler):\n",
    "    input_features,_ = encode_data(input_data,scaler)\n",
    "    predictions = predictor.predict(input_features)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Evaluate\n",
    "evaluate_model(boosting_model, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does this accuracy compare with your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This disciplined workflow makes it easy to try other models.  \n",
    "This [cheat sheet](https://www.analyticsvidhya.com/infographics/Scikit-Learn-Infographic.pdf) provides a list of models supported by scikit learn.  \n",
    "Let's try two more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Errors\n",
    "\n",
    "- Explore the predictions and errors in this Gradient Boosting model. \n",
    "- Are the outliers the same as in the previous models you've fitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Random Forest\n",
    "\n",
    "Implement a model using the same workflow as above with [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Hint: to avoid a warning regarding argument defaults, set `n_estimators = 100` within `RandomForestRegressor()`.\n",
    "\n",
    "How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Support Vector Regression\n",
    "\n",
    "Implement a model using [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Hint: to avoid a warning regarding argument defaults, set `gamma='auto'` within `SVR()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this performance surprising? Have a quick look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) - any ideas how you might improve its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Algorithm hyperparameters (optional)\n",
    "\n",
    "Each of these algorithms has various hyperparameters that can be adjusted to fine tune the model.\n",
    "\n",
    "Hyperparameters are the options used to direct the learning process. Parameters are the learned model coefficients.\n",
    "\n",
    "- Gradient Boosting\n",
    "    - Main tuning hyperparameters:\n",
    "         - `n_estimators`\n",
    "         - `learning_rate`\n",
    "         - `max_depth`\n",
    "- Random Forests\n",
    "    - Main tuning hyperparameters:\n",
    "         - `n_estimators`\n",
    "         - `max_features`\n",
    "         - `max_depth`\n",
    "- Support Vector Regression\n",
    "    - Main tuning hyperparameters:\n",
    "        - `C`\n",
    "        - `epsilon`\n",
    "        - `kernel`\n",
    "        \n",
    "        \n",
    "- Different algorithms respond to hyperparameter adjustments differently. For this data problem, the performance of the SVR model is *highly dependent* on the values of the hyperparameters. This is what can improve the SVR model to closer to the accuracy of the others.\n",
    "\n",
    "\n",
    "- Have a look at the documentation to see what each of these parameters do.\n",
    "- What are their default values?\n",
    "- Play around with changing these hyperparameters to see if you can improve the model accuracies.\n",
    "- For the SVR model - try much larger values for `epsilon` and `C`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
